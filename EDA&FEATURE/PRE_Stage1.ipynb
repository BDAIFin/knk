{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57389f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "884418d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nakyung/projects/BDAIFin/EDA&FEATURE'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faedcde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../DATA/dataset/TRAIN_stage1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a4aa66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_cols: 15 cat_cols: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "LABEL_COL = \"fraud\"  \n",
    "\n",
    "y = df[LABEL_COL].astype(int)\n",
    "X = df.drop(columns=[LABEL_COL])\n",
    "\n",
    "# 1) 컬럼 타입 자동 추정\n",
    "\n",
    "cat_cols = [c for c in X.columns if str(X[c].dtype) in (\"object\", \"category\")]\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "print(\"num_cols:\", len(num_cols), \"cat_cols:\", len(cat_cols))\n",
    "\n",
    "\n",
    "# 2) Train/Valid split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 3) 간단 전처리 파이프\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler(with_mean=False)), \n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_pipe, num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2948ba42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[533]\tvalid's auc: 0.896741\tvalid's average_precision: 0.0375346\n",
      "best_iter: 533\n",
      "best_score: defaultdict(<class 'collections.OrderedDict'>, {'valid': OrderedDict([('auc', np.float64(0.8967412943584248)), ('average_precision', np.float64(0.037534598363610956))])})\n",
      "LGB AUC: 0.8967412943584248\n",
      "LGB PR-AUC: 0.03753459836361088\n",
      "\n",
      "Computing SHAP (LightGBM native)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44818497bc154bf7a01582540d71dc99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top SHAP features:\n",
      " tx_hour                   0.703894\n",
      "hour_circular_distance    0.463239\n",
      "hour_cos                  0.446013\n",
      "mcc_highrisk_90           0.440688\n",
      "log_abs_amount            0.440157\n",
      "cos_shift                 0.229152\n",
      "is_refund                 0.218054\n",
      "is_highrisk_weekday       0.180360\n",
      "sin_shift                 0.150146\n",
      "tx_month                  0.145714\n",
      "hour_sin                  0.138593\n",
      "has_error                 0.005450\n",
      "err_bad_cvv               0.002073\n",
      "err_bad_card_number       0.001825\n",
      "err_bad_expiration        0.001203\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "X_train_lgb = X_train.copy()\n",
    "X_valid_lgb = X_valid.copy()\n",
    "\n",
    "dtrain = lgb.Dataset(X_train_lgb, label=y_train, free_raw_data=False)\n",
    "dvalid = lgb.Dataset(X_valid_lgb, label=y_valid, free_raw_data=False)\n",
    "\n",
    "params = dict(\n",
    "    objective=\"binary\",\n",
    "    metric=[\"auc\", \"average_precision\"],\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=64,\n",
    "    min_data_in_leaf=200,\n",
    "    feature_fraction=0.8,\n",
    "    bagging_fraction=0.8,\n",
    "    bagging_freq=1,\n",
    "    verbosity=-1,\n",
    ")\n",
    "\n",
    "bst = lgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=5000,\n",
    "    valid_sets=[dvalid],\n",
    "    valid_names=[\"valid\"],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(200, verbose=True),\n",
    "        lgb.log_evaluation(0),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"best_iter:\", bst.best_iteration)\n",
    "print(\"best_score:\", bst.best_score)\n",
    "\n",
    "pred_valid = bst.predict(X_valid_lgb, num_iteration=bst.best_iteration)\n",
    "print(\"LGB AUC:\", roc_auc_score(y_valid, pred_valid))\n",
    "print(\"LGB PR-AUC:\", average_precision_score(y_valid, pred_valid))\n",
    "\n",
    "\n",
    "# SHAP with tqdm (batch version)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sv = X_valid_lgb\n",
    "\n",
    "batch_size = 2000\n",
    "all_contrib = []\n",
    "\n",
    "print(\"\\nComputing SHAP (LightGBM native)...\")\n",
    "\n",
    "for i in tqdm(range(0, len(sv), batch_size)):\n",
    "    batch = sv.iloc[i:i+batch_size]\n",
    "    contrib = bst.predict(batch, pred_contrib=True)\n",
    "    all_contrib.append(contrib[:, :-1])  # 마지막 열은 bias\n",
    "\n",
    "shap_values = np.vstack(all_contrib)\n",
    "\n",
    "imp = pd.Series(\n",
    "    np.abs(shap_values).mean(axis=0),\n",
    "    index=sv.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop SHAP features:\\n\", imp.head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22f91b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13841322aa784b269380e49513646861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 1:   0%|          | 0/1042 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] valid AUC=0.87273  PR-AUC=0.02141\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afbf8b793ff47cb8c033ef8f579e8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 2:   0%|          | 0/1042 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2] valid AUC=0.88169  PR-AUC=0.02948\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf16b5e568004ddc92d8c1aa240cce35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 3:   0%|          | 0/1042 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 3] valid AUC=0.88292  PR-AUC=0.02908\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e52248b0c64a05911f4e24bb3ed564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 4:   0%|          | 0/1042 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 4] valid AUC=0.88676  PR-AUC=0.03287\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946012cb542f41b1b9948487745569ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 5:   0%|          | 0/1042 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 5] valid AUC=0.88789  PR-AUC=0.03510\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ce0007d9f7485798f8092cc0efdcc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "extract attention:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Attention features:\n",
      " log_abs_amount            0.156898\n",
      "hour_sin                  0.146947\n",
      "sin_shift                 0.120691\n",
      "cos_shift                 0.085613\n",
      "mcc_highrisk_90           0.081791\n",
      "hour_cos                  0.077360\n",
      "is_refund                 0.076557\n",
      "tx_hour                   0.053810\n",
      "tx_month                  0.040203\n",
      "err_bad_expiration        0.034823\n",
      "err_bad_card_number       0.034044\n",
      "is_highrisk_weekday       0.026904\n",
      "has_error                 0.021560\n",
      "hour_circular_distance    0.015177\n",
      "err_bad_cvv               0.008864\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "feature_names = list(X_train.columns)\n",
    "n_features = len(feature_names)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xtr = scaler.fit_transform(X_train.values.astype(np.float32))\n",
    "Xva = scaler.transform(X_valid.values.astype(np.float32))\n",
    "\n",
    "ytr = y_train.values.astype(np.float32)\n",
    "yva = y_valid.values.astype(np.float32)\n",
    "\n",
    "class TabDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "train_loader = DataLoader(TabDataset(Xtr, ytr), batch_size=4096, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(TabDataset(Xva, yva), batch_size=8192, shuffle=False, num_workers=0)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1) Attention layer (weights 반환)\n",
    "\n",
    "class AttnEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model=64, n_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, return_attn=False):\n",
    "        # x: [B, T, D]\n",
    "        attn_out, attn_w = self.mha(x, x, x, need_weights=True, average_attn_weights=False)\n",
    "        x = self.ln1(x + attn_out)\n",
    "        x = self.ln2(x + self.ff(x))\n",
    "        if return_attn:\n",
    "            # attn_w: [B, heads, T, T]\n",
    "            return x, attn_w\n",
    "        return x\n",
    "\n",
    "\n",
    "# 2) Tabular Transformer (CLS 토큰 사용)\n",
    "\n",
    "class TabularAttentionModel(nn.Module):\n",
    "    def __init__(self, n_features, d_model=64, n_heads=4, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # feature별 1->d 투영 (각 feature마다 별도의 linear)\n",
    "        self.feat_proj = nn.ModuleList([nn.Linear(1, d_model) for _ in range(n_features)])\n",
    "\n",
    "        # CLS token\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        nn.init.normal_(self.cls, std=0.02)\n",
    "\n",
    "        self.layers = nn.ModuleList([AttnEncoderLayer(d_model, n_heads, dropout) for _ in range(n_layers)])\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, X, return_attn=False):\n",
    "        # X: [B, F]\n",
    "        B, F = X.shape\n",
    "\n",
    "        # feature tokens 만들기: [B, F, D]\n",
    "        toks = []\n",
    "        for j in range(F):\n",
    "            xj = X[:, j:j+1]                 # [B, 1]\n",
    "            toks.append(self.feat_proj[j](xj))  # [B, D]\n",
    "        tok = torch.stack(toks, dim=1)       # [B, F, D]\n",
    "\n",
    "        # CLS 붙이기: [B, 1+F, D]\n",
    "        cls = self.cls.expand(B, -1, -1)\n",
    "        x = torch.cat([cls, tok], dim=1)\n",
    "\n",
    "        attn_all = []\n",
    "        for layer in self.layers:\n",
    "            if return_attn:\n",
    "                x, attn = layer(x, return_attn=True)\n",
    "                attn_all.append(attn)\n",
    "            else:\n",
    "                x = layer(x, return_attn=False)\n",
    "\n",
    "        # CLS representation으로 예측\n",
    "        cls_repr = x[:, 0, :]               # [B, D]\n",
    "        logit = self.head(cls_repr).squeeze(1)\n",
    "\n",
    "        if return_attn:\n",
    "            return logit, attn_all  # list of [B, heads, T, T]\n",
    "        return logit\n",
    "\n",
    "\n",
    "# 3) 학습 루프\n",
    "\n",
    "model = TabularAttentionModel(n_features=n_features, d_model=64, n_heads=4, n_layers=2, dropout=0.1).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def eval_model():\n",
    "    model.eval()\n",
    "    ps, ys = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in valid_loader:\n",
    "            xb = xb.to(device)\n",
    "            logit = model(xb)\n",
    "            prob = torch.sigmoid(logit).cpu().numpy()\n",
    "            ps.append(prob)\n",
    "            ys.append(yb.numpy())\n",
    "    p = np.concatenate(ps)\n",
    "    t = np.concatenate(ys)\n",
    "    return roc_auc_score(t, p), average_precision_score(t, p)\n",
    "\n",
    "EPOCHS = 5\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"train epoch {epoch}\", leave=False)\n",
    "    for xb, yb in pbar:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logit = model(xb)\n",
    "        loss = loss_fn(logit, yb)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        pbar.set_postfix(loss=float(loss.detach().cpu()))\n",
    "\n",
    "    auc, pr = eval_model()\n",
    "    print(f\"[epoch {epoch}] valid AUC={auc:.5f}  PR-AUC={pr:.5f}\")\n",
    "\n",
    "\n",
    "# 4) Attention 추출 → 컬럼 중요도\n",
    "# - CLS(0번 토큰)에서 각 feature 토큰으로 가는 attention을 사용\n",
    "\n",
    "def extract_feature_attention_importance(model, loader, n_batches=50):\n",
    "    model.eval()\n",
    "    # 누적: feature별 attention 합\n",
    "    att_sum = np.zeros((n_features,), dtype=np.float64)\n",
    "    cnt = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for b, (xb, yb) in enumerate(tqdm(loader, desc=\"extract attention\", total=min(n_batches, len(loader)))):\n",
    "            if b >= n_batches:\n",
    "                break\n",
    "            xb = xb.to(device)\n",
    "\n",
    "            logit, attn_all = model(xb, return_attn=True)\n",
    "            attn = attn_all[-1]  # [B, heads, T, T]\n",
    "\n",
    "            # CLS -> feature 토큰 attention: query=0, key=1..F\n",
    "            # shape: [B, heads, F]\n",
    "            cls_to_feat = attn[:, :, 0, 1:]  \n",
    "\n",
    "            # heads 평균, batch 평균 → [F]\n",
    "            score = cls_to_feat.mean(dim=1).mean(dim=0).cpu().numpy()\n",
    "            att_sum += score\n",
    "            cnt += 1\n",
    "\n",
    "    att_mean = att_sum / max(cnt, 1)\n",
    "    imp = pd.Series(att_mean, index=feature_names).sort_values(ascending=False)\n",
    "    return imp\n",
    "\n",
    "att_imp = extract_feature_attention_importance(model, valid_loader, n_batches=50)\n",
    "print(\"\\nTop Attention features:\\n\", att_imp.head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3386aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        shap_mean_abs  attn_cls2feat  shap_rank  attn_rank  \\\n",
      "tx_hour                      0.703894       0.053810        1.0        8.0   \n",
      "hour_circular_distance       0.463239       0.015177        2.0       14.0   \n",
      "hour_cos                     0.446013       0.077360        3.0        6.0   \n",
      "mcc_highrisk_90              0.440688       0.081791        4.0        5.0   \n",
      "log_abs_amount               0.440157       0.156898        5.0        1.0   \n",
      "cos_shift                    0.229152       0.085613        6.0        4.0   \n",
      "is_refund                    0.218054       0.076557        7.0        7.0   \n",
      "is_highrisk_weekday          0.180360       0.026904        8.0       12.0   \n",
      "sin_shift                    0.150146       0.120691        9.0        3.0   \n",
      "tx_month                     0.145714       0.040203       10.0        9.0   \n",
      "hour_sin                     0.138593       0.146947       11.0        2.0   \n",
      "has_error                    0.005450       0.021560       12.0       13.0   \n",
      "err_bad_cvv                  0.002073       0.008864       13.0       15.0   \n",
      "err_bad_card_number          0.001825       0.034044       14.0       11.0   \n",
      "err_bad_expiration           0.001203       0.034823       15.0       10.0   \n",
      "\n",
      "                        rank_gap  \n",
      "tx_hour                      7.0  \n",
      "hour_circular_distance      12.0  \n",
      "hour_cos                     3.0  \n",
      "mcc_highrisk_90              1.0  \n",
      "log_abs_amount              -4.0  \n",
      "cos_shift                   -2.0  \n",
      "is_refund                    0.0  \n",
      "is_highrisk_weekday          4.0  \n",
      "sin_shift                   -6.0  \n",
      "tx_month                    -1.0  \n",
      "hour_sin                    -9.0  \n",
      "has_error                    1.0  \n",
      "err_bad_cvv                  2.0  \n",
      "err_bad_card_number         -3.0  \n",
      "err_bad_expiration          -5.0  \n"
     ]
    }
   ],
   "source": [
    "compare = pd.DataFrame({\n",
    "    \"shap_mean_abs\": imp,        \n",
    "    \"attn_cls2feat\": att_imp\n",
    "}).fillna(0.0)\n",
    "\n",
    "compare[\"shap_rank\"] = compare[\"shap_mean_abs\"].rank(ascending=False, method=\"min\")\n",
    "compare[\"attn_rank\"] = compare[\"attn_cls2feat\"].rank(ascending=False, method=\"min\")\n",
    "compare[\"rank_gap\"] = compare[\"attn_rank\"] - compare[\"shap_rank\"]\n",
    "\n",
    "print(compare.sort_values(\"shap_mean_abs\", ascending=False).head(40))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313db376",
   "metadata": {},
   "source": [
    "### High\n",
    "\n",
    "- tx_hour\n",
    "- hour_cos\n",
    "- hour_sin\n",
    "- log_abs_amount\n",
    "- mcc_highrisk_90\n",
    "- cos_shift\n",
    "- sin_shift\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e6e001",
   "metadata": {},
   "source": [
    "> 시간 & 금액이 강하게 작용\\\n",
    "> Error 계열은 약함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91042306",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7de1fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "params = dict(\n",
    "    objective=\"binary\",\n",
    "    metric=\"auc\",\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=64,\n",
    "    min_data_in_leaf=200,\n",
    "    feature_fraction=0.8,\n",
    "    bagging_fraction=0.8,\n",
    "    bagging_freq=1,\n",
    "    verbosity=-1,\n",
    ")\n",
    "\n",
    "def train_eval(X_tr, y_tr, X_va, y_va):\n",
    "    dtrain = lgb.Dataset(X_tr, label=y_tr, free_raw_data=False)\n",
    "    dvalid = lgb.Dataset(X_va, label=y_va, free_raw_data=False)\n",
    "\n",
    "    bst = lgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=3000,\n",
    "        valid_sets=[dvalid],\n",
    "        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)],\n",
    "    )\n",
    "\n",
    "    pred = bst.predict(X_va, num_iteration=bst.best_iteration)\n",
    "\n",
    "    return {\n",
    "        \"auc\": roc_auc_score(y_va, pred),\n",
    "        \"prauc\": average_precision_score(y_va, pred),\n",
    "        \"best_iter\": bst.best_iteration,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99bb12c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[598]\tvalid_0's auc: 0.897014\n",
      "BASE: {'auc': 0.897013976804593, 'prauc': 0.037417840743750264, 'best_iter': 598}\n"
     ]
    }
   ],
   "source": [
    "base_result = train_eval(X_train, y_train, X_valid, y_valid)\n",
    "print(\"BASE:\", base_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6ccf1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd678fdaf2fc4060ace289682e0d0519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[471]\tvalid_0's auc: 0.89614\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[400]\tvalid_0's auc: 0.896962\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[416]\tvalid_0's auc: 0.895368\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[142]\tvalid_0's auc: 0.894249\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[161]\tvalid_0's auc: 0.891889\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[567]\tvalid_0's auc: 0.896626\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[445]\tvalid_0's auc: 0.892308\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[419]\tvalid_0's auc: 0.880192\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[355]\tvalid_0's auc: 0.896945\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[433]\tvalid_0's auc: 0.896971\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[422]\tvalid_0's auc: 0.895872\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[586]\tvalid_0's auc: 0.898957\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[126]\tvalid_0's auc: 0.882352\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[598]\tvalid_0's auc: 0.85249\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[424]\tvalid_0's auc: 0.892179\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dropped_feature</th>\n",
       "      <th>auc_drop</th>\n",
       "      <th>prauc_drop</th>\n",
       "      <th>auc</th>\n",
       "      <th>prauc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mcc_highrisk_90</td>\n",
       "      <td>0.044524</td>\n",
       "      <td>0.018001</td>\n",
       "      <td>0.852490</td>\n",
       "      <td>0.019417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>log_abs_amount</td>\n",
       "      <td>0.016822</td>\n",
       "      <td>0.006314</td>\n",
       "      <td>0.880192</td>\n",
       "      <td>0.031104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hour_circular_distance</td>\n",
       "      <td>0.014662</td>\n",
       "      <td>0.008045</td>\n",
       "      <td>0.882352</td>\n",
       "      <td>0.029373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tx_month</td>\n",
       "      <td>0.005125</td>\n",
       "      <td>0.004357</td>\n",
       "      <td>0.891889</td>\n",
       "      <td>0.033061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>is_highrisk_weekday</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.003664</td>\n",
       "      <td>0.892179</td>\n",
       "      <td>0.033754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>is_refund</td>\n",
       "      <td>0.004706</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.035315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>err_bad_cvv</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.006715</td>\n",
       "      <td>0.894249</td>\n",
       "      <td>0.030703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>err_bad_expiration</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.895368</td>\n",
       "      <td>0.035784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sin_shift</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>-0.001022</td>\n",
       "      <td>0.895872</td>\n",
       "      <td>0.038440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>has_error</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.896140</td>\n",
       "      <td>0.037056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tx_hour</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>-0.002522</td>\n",
       "      <td>0.896626</td>\n",
       "      <td>0.039940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hour_sin</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-0.000513</td>\n",
       "      <td>0.896945</td>\n",
       "      <td>0.037930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>err_bad_card_number</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.002487</td>\n",
       "      <td>0.896962</td>\n",
       "      <td>0.034931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hour_cos</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>-0.000207</td>\n",
       "      <td>0.896971</td>\n",
       "      <td>0.037625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cos_shift</td>\n",
       "      <td>-0.001943</td>\n",
       "      <td>-0.000937</td>\n",
       "      <td>0.898957</td>\n",
       "      <td>0.038355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dropped_feature  auc_drop  prauc_drop       auc     prauc\n",
       "13         mcc_highrisk_90  0.044524    0.018001  0.852490  0.019417\n",
       "7           log_abs_amount  0.016822    0.006314  0.880192  0.031104\n",
       "12  hour_circular_distance  0.014662    0.008045  0.882352  0.029373\n",
       "4                 tx_month  0.005125    0.004357  0.891889  0.033061\n",
       "14     is_highrisk_weekday  0.004835    0.003664  0.892179  0.033754\n",
       "6                is_refund  0.004706    0.002103  0.892308  0.035315\n",
       "3              err_bad_cvv  0.002765    0.006715  0.894249  0.030703\n",
       "2       err_bad_expiration  0.001646    0.001634  0.895368  0.035784\n",
       "10               sin_shift  0.001142   -0.001022  0.895872  0.038440\n",
       "0                has_error  0.000873    0.000362  0.896140  0.037056\n",
       "5                  tx_hour  0.000388   -0.002522  0.896626  0.039940\n",
       "8                 hour_sin  0.000069   -0.000513  0.896945  0.037930\n",
       "1      err_bad_card_number  0.000052    0.002487  0.896962  0.034931\n",
       "9                 hour_cos  0.000043   -0.000207  0.896971  0.037625\n",
       "11               cos_shift -0.001943   -0.000937  0.898957  0.038355"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_one_results = []\n",
    "\n",
    "for col in tqdm(X_train.columns):\n",
    "    cols = [c for c in X_train.columns if c != col]\n",
    "\n",
    "    res = train_eval(\n",
    "        X_train[cols],\n",
    "        y_train,\n",
    "        X_valid[cols],\n",
    "        y_valid,\n",
    "    )\n",
    "\n",
    "    drop_one_results.append({\n",
    "        \"dropped_feature\": col,\n",
    "        \"auc_drop\": base_result[\"auc\"] - res[\"auc\"],\n",
    "        \"prauc_drop\": base_result[\"prauc\"] - res[\"prauc\"],\n",
    "        \"auc\": res[\"auc\"],\n",
    "        \"prauc\": res[\"prauc\"],\n",
    "    })\n",
    "\n",
    "drop_one_df = pd.DataFrame(drop_one_results)\\\n",
    "    .sort_values(\"auc_drop\", ascending=False)\n",
    "\n",
    "drop_one_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e07e58",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39555668",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU-CAT)",
   "language": "python",
   "name": "gpu-cat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
