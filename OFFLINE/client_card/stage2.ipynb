{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1b36dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, ParameterSampler\n",
    "\n",
    "import joblib\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LGB = True\n",
    "except Exception:\n",
    "    HAS_LGB = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89cdc94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LABEL = \"fraud\"\n",
    "\n",
    "TRAIN_PATH = \"../../DATA/dataset/TRAIN_stage2\"\n",
    "TEST_PATH  = \"../../DATA/dataset/TEST_stage2\"\n",
    "\n",
    "OUT_DIR = \"artifacts/stage2_models\"\n",
    "OUT_DIR_METRICS = \"artifacts/stage2_metrics\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bed3fc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(OUT_DIR_METRICS).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdc6c03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_stage_df(path: str, label: str = LABEL):\n",
    "    df = pd.read_parquet(path)\n",
    "    if label not in df.columns:\n",
    "        raise KeyError(f\"Missing label column: {label}\")\n",
    "    X = df.drop(columns=[label])\n",
    "    y = df[label].astype(np.int8).to_numpy()\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34f522b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_metrics(y_true, score, top_pct_list=(0.001, 0.002, 0.005, 0.01)):\n",
    "    y = np.asarray(y_true).astype(int)\n",
    "    s = np.asarray(score).astype(float)\n",
    "    n = len(y)\n",
    "    base_rate = float(y.mean()) if n else np.nan\n",
    "    order = np.argsort(-s)\n",
    "    y_sorted = y[order]\n",
    "\n",
    "    rows = []\n",
    "    for p in top_pct_list:\n",
    "        k = max(int(np.ceil(n * p)), 1)\n",
    "        top_y = y_sorted[:k]\n",
    "        prec = float(top_y.mean())\n",
    "        rec = float(top_y.sum() / max(y.sum(), 1))\n",
    "        lift = float(prec / base_rate) if base_rate and base_rate > 0 else np.nan\n",
    "        rows.append({\"top_pct\": p, \"k\": k, \"precision\": prec, \"recall\": rec, \"lift\": lift, \"base_rate\": base_rate})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def evaluate_metrics(y_true, score):\n",
    "    return {\n",
    "        \"auc\": float(roc_auc_score(y_true, score)),\n",
    "        \"prauc\": float(average_precision_score(y_true, score)),\n",
    "        \"base_rate\": float(np.mean(y_true)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84b939d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_logit(X_tr, y_tr):\n",
    "    model = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            solver=\"lbfgs\",\n",
    "            max_iter=2000,\n",
    "            class_weight=\"balanced\",\n",
    "        ))\n",
    "    ])\n",
    "    model.fit(X_tr, y_tr)\n",
    "    return model\n",
    "\n",
    "\n",
    "def fit_hgb(X_tr, y_tr):\n",
    "    model = HistGradientBoostingClassifier(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        max_iter=400,\n",
    "        min_samples_leaf=200,\n",
    "        l2_regularization=0.0,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "    return model\n",
    "\n",
    "\n",
    "def fit_lgb_small(X_tr, y_tr):\n",
    "    if not HAS_LGB:\n",
    "        raise RuntimeError(\"lightgbm is not available in this environment.\")\n",
    "    model = lgb.LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        max_depth=6,\n",
    "        min_data_in_leaf=300,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_score(model, X):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        s = model.decision_function(X)\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    return model.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64523cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (5312525, 17) test: (928867, 17)\n",
      "base_rate train: 0.0014471461310770302 test: 0.002624702998384053\n"
     ]
    }
   ],
   "source": [
    "X_tr, y_tr = load_stage_df(TRAIN_PATH)\n",
    "X_te, y_te = load_stage_df(TEST_PATH)\n",
    "\n",
    "print(\"train:\", X_tr.shape, \"test:\", X_te.shape)\n",
    "print(\"base_rate train:\", float(y_tr.mean()), \"test:\", float(y_te.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aa6bca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c10ea9017b4211a3e67d0e4dbdafa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Stage1 models:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[logit] threshold=quantile(0.99) -> top 1% as positive\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9989    0.9915    0.9952    926429\n",
      "           1     0.1571    0.5984    0.2488      2438\n",
      "\n",
      "    accuracy                         0.9905    928867\n",
      "   macro avg     0.5780    0.7950    0.6220    928867\n",
      "weighted avg     0.9967    0.9905    0.9933    928867\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[hgb] threshold=quantile(0.99) -> top 1% as positive\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9985    0.9911    0.9948    926429\n",
      "           1     0.1159    0.4446    0.1839      2438\n",
      "\n",
      "    accuracy                         0.9896    928867\n",
      "   macro avg     0.5572    0.7179    0.5894    928867\n",
      "weighted avg     0.9962    0.9896    0.9927    928867\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=300, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=300\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=300, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=300\n",
      "[LightGBM] [Info] Number of positive: 7688, number of negative: 5304837\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081949 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1686\n",
      "[LightGBM] [Info] Number of data points in the train set: 5312525, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001447 -> initscore=-6.536714\n",
      "[LightGBM] [Info] Start training from score -6.536714\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=300, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=300\n",
      "\n",
      "================================================================================\n",
      "[lgb_small] threshold=quantile(0.99) -> top 1% as positive\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9989    0.9915    0.9952    926429\n",
      "           1     0.1524    0.5808    0.2415      2438\n",
      "\n",
      "    accuracy                         0.9904    928867\n",
      "   macro avg     0.5757    0.7862    0.6183    928867\n",
      "weighted avg     0.9967    0.9904    0.9932    928867\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc</th>\n",
       "      <th>prauc</th>\n",
       "      <th>base_rate</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.945793</td>\n",
       "      <td>0.434644</td>\n",
       "      <td>0.002625</td>\n",
       "      <td>lgb_small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.802043</td>\n",
       "      <td>0.280938</td>\n",
       "      <td>0.002625</td>\n",
       "      <td>hgb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.947069</td>\n",
       "      <td>0.252247</td>\n",
       "      <td>0.002625</td>\n",
       "      <td>logit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        auc     prauc  base_rate      model\n",
       "0  0.945793  0.434644   0.002625  lgb_small\n",
       "1  0.802043  0.280938   0.002625        hgb\n",
       "2  0.947069  0.252247   0.002625      logit"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "candidates = [\n",
    "    (\"logit\", fit_logit),\n",
    "    (\"hgb\", fit_hgb),\n",
    "]\n",
    "if HAS_LGB:\n",
    "    candidates.append((\"lgb_small\", fit_lgb_small))\n",
    "\n",
    "results = []\n",
    "topk_all = {}\n",
    "reports = {}\n",
    "\n",
    "for name, fit_fn in tqdm(candidates, desc=\"Training Stage1 models\"):\n",
    "    model = fit_fn(X_tr, y_tr)\n",
    "    score_te = predict_score(model, X_te)\n",
    "\n",
    "    m = evaluate_metrics(y_te, score_te)\n",
    "    m[\"model\"] = name\n",
    "    results.append(m)\n",
    "\n",
    "    topk = topk_metrics(y_te, score_te)\n",
    "    topk_all[name] = topk\n",
    "    topk.to_csv(Path(OUT_DIR_METRICS) / f\"{name}_topk.csv\", index=False)\n",
    "\n",
    "    joblib.dump(model, Path(OUT_DIR) / f\"{name}.joblib\")\n",
    "    np.save(Path(OUT_DIR_METRICS) / f\"{name}_test_scores.npy\", score_te)\n",
    "\n",
    "    thr = np.quantile(score_te, 0.99)\n",
    "    y_pred = (score_te >= thr).astype(int)\n",
    "\n",
    "    rep_txt = classification_report(y_te, y_pred, digits=4)\n",
    "    reports[name] = rep_txt\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"[{name}] threshold=quantile(0.99) -> top 1% as positive\")\n",
    "    print(rep_txt)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values([\"prauc\", \"auc\"], ascending=False).reset_index(drop=True)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e9296b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e95a2f",
   "metadata": {},
   "source": [
    "stage1에서 걸러진 데이터만 2차로 거른다면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36668531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train2: (5312525, 19)\n",
      "test2: (928867, 19)\n",
      "test2_pass: (9146, 19)\n",
      "pass_rate_in_test2: 0.009846404275316058\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PASS_PATH = \"../transaction/artifacts/stage1_pass_ids_test.parquet\"\n",
    "TRAIN2_PATH = \"../../DATA/dataset/TRAIN_stage2\"\n",
    "TEST2_PATH  = \"../../DATA/dataset/TEST_stage2\"\n",
    "\n",
    "pass_ids = pd.read_parquet(PASS_PATH)[\"id\"].astype(\"int64\")\n",
    "\n",
    "train2 = pd.read_parquet(TRAIN2_PATH)\n",
    "test2  = pd.read_parquet(TEST2_PATH)\n",
    "\n",
    "test2_pass = test2[test2[\"id\"].isin(pass_ids)].copy()\n",
    "\n",
    "print(\"train2:\", train2.shape)\n",
    "print(\"test2:\", test2.shape)\n",
    "print(\"test2_pass:\", test2_pass.shape)\n",
    "print(\"pass_rate_in_test2:\", len(test2_pass) / len(test2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bb78a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = \"artifacts/stage2_models\"\n",
    "OUT_DIR_METRICS = \"artifacts/stage2_metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b96292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(OUT_DIR_METRICS).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "978f4acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = \"fraud\"\n",
    "\n",
    "def load_stage_df(df, label: str = LABEL, id_col: str = \"id\"):\n",
    "    df = df.copy()\n",
    "\n",
    "    if label not in df.columns:\n",
    "        raise KeyError(f\"Missing label column: {label}\")\n",
    "\n",
    "    drop_cols = [label]\n",
    "    if id_col in df.columns:\n",
    "        drop_cols.append(id_col)\n",
    "\n",
    "    y = df[label].astype(np.int8).to_numpy()\n",
    "    X = df.drop(columns=drop_cols)\n",
    "\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c128d675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_metrics(y_true, score, top_pct_list=(0.001, 0.002, 0.005, 0.01)):\n",
    "    y = np.asarray(y_true).astype(int)\n",
    "    s = np.asarray(score).astype(float)\n",
    "    n = len(y)\n",
    "    base_rate = float(y.mean()) if n else np.nan\n",
    "    order = np.argsort(-s)\n",
    "    y_sorted = y[order]\n",
    "\n",
    "    rows = []\n",
    "    for p in top_pct_list:\n",
    "        k = max(int(np.ceil(n * p)), 1)\n",
    "        top_y = y_sorted[:k]\n",
    "        prec = float(top_y.mean())\n",
    "        rec = float(top_y.sum() / max(y.sum(), 1))\n",
    "        lift = float(prec / base_rate) if base_rate and base_rate > 0 else np.nan\n",
    "        rows.append({\"top_pct\": p, \"k\": k, \"precision\": prec, \"recall\": rec, \"lift\": lift, \"base_rate\": base_rate})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def evaluate_metrics(y_true, score):\n",
    "    return {\n",
    "        \"auc\": float(roc_auc_score(y_true, score)),\n",
    "        \"prauc\": float(average_precision_score(y_true, score)),\n",
    "        \"base_rate\": float(np.mean(y_true)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f04e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_logit(X_tr, y_tr):\n",
    "    model = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            solver=\"lbfgs\",\n",
    "            max_iter=2000,\n",
    "            class_weight=\"balanced\",\n",
    "        ))\n",
    "    ])\n",
    "    model.fit(X_tr, y_tr)\n",
    "    return model\n",
    "\n",
    "\n",
    "def fit_hgb(X_tr, y_tr):\n",
    "    model = HistGradientBoostingClassifier(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        max_iter=400,\n",
    "        min_samples_leaf=200,\n",
    "        l2_regularization=0.0,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "    return model\n",
    "\n",
    "\n",
    "def fit_lgb_small(X_tr, y_tr):\n",
    "    if not HAS_LGB:\n",
    "        raise RuntimeError(\"lightgbm is not available in this environment.\")\n",
    "    model = lgb.LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        max_depth=6,\n",
    "        min_data_in_leaf=300,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_score(model, X):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        s = model.decision_function(X)\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    return model.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25160559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (5312525, 17) test: (9146, 17)\n",
      "base_rate train: 0.0014471461310770302 test: 0.15908593920839711\n"
     ]
    }
   ],
   "source": [
    "X_tr, y_tr = load_stage_df(train2)\n",
    "X_te, y_te = load_stage_df(test2_pass)\n",
    "\n",
    "print(\"train:\", X_tr.shape, \"test:\", X_te.shape)\n",
    "print(\"base_rate train:\", float(y_tr.mean()), \"test:\", float(y_te.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c384cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67978fcb186f4db38c6021e24653918a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Stage1 models:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[logit] threshold=quantile(0.99) -> top 1% as positive\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8461    0.9961    0.9150      7691\n",
      "           1     0.6739    0.0426    0.0802      1455\n",
      "\n",
      "    accuracy                         0.8444      9146\n",
      "   macro avg     0.7600    0.5194    0.4976      9146\n",
      "weighted avg     0.8187    0.8444    0.7822      9146\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[hgb] threshold=quantile(0.99) -> top 1% as positive\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9219    0.9555    0.9384      7691\n",
      "           1     0.7087    0.5718    0.6329      1455\n",
      "\n",
      "    accuracy                         0.8945      9146\n",
      "   macro avg     0.8153    0.7637    0.7857      9146\n",
      "weighted avg     0.8879    0.8945    0.8898      9146\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=300, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=300\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=300, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=300\n",
      "[LightGBM] [Info] Number of positive: 7688, number of negative: 5304837\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085054 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1686\n",
      "[LightGBM] [Info] Number of data points in the train set: 5312525, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001447 -> initscore=-6.536714\n",
      "[LightGBM] [Info] Start training from score -6.536714\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=300, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=300\n",
      "\n",
      "================================================================================\n",
      "[lgb_small] threshold=quantile(0.99) -> top 1% as positive\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8533    0.9992    0.9205      7691\n",
      "           1     0.9571    0.0921    0.1680      1455\n",
      "\n",
      "    accuracy                         0.8549      9146\n",
      "   macro avg     0.9052    0.5457    0.5443      9146\n",
      "weighted avg     0.8698    0.8549    0.8008      9146\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc</th>\n",
       "      <th>prauc</th>\n",
       "      <th>base_rate</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.891103</td>\n",
       "      <td>0.716675</td>\n",
       "      <td>0.159086</td>\n",
       "      <td>lgb_small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.691538</td>\n",
       "      <td>0.510391</td>\n",
       "      <td>0.159086</td>\n",
       "      <td>hgb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.842729</td>\n",
       "      <td>0.447422</td>\n",
       "      <td>0.159086</td>\n",
       "      <td>logit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        auc     prauc  base_rate      model\n",
       "0  0.891103  0.716675   0.159086  lgb_small\n",
       "1  0.691538  0.510391   0.159086        hgb\n",
       "2  0.842729  0.447422   0.159086      logit"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "candidates = [\n",
    "    (\"logit\", fit_logit),\n",
    "    (\"hgb\", fit_hgb),\n",
    "]\n",
    "if HAS_LGB:\n",
    "    candidates.append((\"lgb_small\", fit_lgb_small))\n",
    "\n",
    "results = []\n",
    "topk_all = {}\n",
    "reports = {}\n",
    "\n",
    "for name, fit_fn in tqdm(candidates, desc=\"Training Stage1 models\"):\n",
    "    model = fit_fn(X_tr, y_tr)\n",
    "    score_te = predict_score(model, X_te)\n",
    "\n",
    "    m = evaluate_metrics(y_te, score_te)\n",
    "    m[\"model\"] = name\n",
    "    results.append(m)\n",
    "\n",
    "    topk = topk_metrics(y_te, score_te)\n",
    "    topk_all[name] = topk\n",
    "    topk.to_csv(Path(OUT_DIR_METRICS) / f\"{name}_topk.csv\", index=False)\n",
    "\n",
    "    joblib.dump(model, Path(OUT_DIR) / f\"{name}.joblib\")\n",
    "    np.save(Path(OUT_DIR_METRICS) / f\"{name}_test_scores.npy\", score_te)\n",
    "\n",
    "    thr = np.quantile(score_te, 0.99)\n",
    "    y_pred = (score_te >= thr).astype(int)\n",
    "\n",
    "    rep_txt = classification_report(y_te, y_pred, digits=4)\n",
    "    reports[name] = rep_txt\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"[{name}] threshold=quantile(0.99) -> top 1% as positive\")\n",
    "    print(rep_txt)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values([\"prauc\", \"auc\"], ascending=False).reset_index(drop=True)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63494cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b361ec153d84717bf70f0572c291287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HGB tuning:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>ok</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>threshold</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_leaf_nodes</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>l2_regularization</th>\n",
       "      <th>max_bins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>True</td>\n",
       "      <td>0.849661</td>\n",
       "      <td>0.602062</td>\n",
       "      <td>0.827425</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>127</td>\n",
       "      <td>200</td>\n",
       "      <td>5.0</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>True</td>\n",
       "      <td>0.848544</td>\n",
       "      <td>0.600687</td>\n",
       "      <td>0.886483</td>\n",
       "      <td>0.05</td>\n",
       "      <td>7.0</td>\n",
       "      <td>63</td>\n",
       "      <td>20</td>\n",
       "      <td>10.0</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>True</td>\n",
       "      <td>0.845930</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.865247</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>127</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>True</td>\n",
       "      <td>0.841851</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.899208</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3.0</td>\n",
       "      <td>63</td>\n",
       "      <td>200</td>\n",
       "      <td>10.0</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>0.836207</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.815635</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.0</td>\n",
       "      <td>63</td>\n",
       "      <td>20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37</td>\n",
       "      <td>True</td>\n",
       "      <td>0.835407</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.749201</td>\n",
       "      <td>0.02</td>\n",
       "      <td>7.0</td>\n",
       "      <td>63</td>\n",
       "      <td>50</td>\n",
       "      <td>10.0</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30</td>\n",
       "      <td>True</td>\n",
       "      <td>0.834608</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.854034</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>0.834601</td>\n",
       "      <td>0.603436</td>\n",
       "      <td>0.889215</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.0</td>\n",
       "      <td>63</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>28</td>\n",
       "      <td>True</td>\n",
       "      <td>0.831181</td>\n",
       "      <td>0.619244</td>\n",
       "      <td>0.833018</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>0.829224</td>\n",
       "      <td>0.624055</td>\n",
       "      <td>0.735924</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>10.0</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trial    ok  precision    recall  threshold  learning_rate  max_depth  \\\n",
       "0     17  True   0.849661  0.602062   0.827425           0.02        NaN   \n",
       "1     29  True   0.848544  0.600687   0.886483           0.05        7.0   \n",
       "2     25  True   0.845930  0.600000   0.865247           0.02        NaN   \n",
       "3     26  True   0.841851  0.600000   0.899208           0.05        3.0   \n",
       "4      9  True   0.836207  0.600000   0.815635           0.02        5.0   \n",
       "5     37  True   0.835407  0.600000   0.749201           0.02        7.0   \n",
       "6     30  True   0.834608  0.600000   0.854034           0.02        NaN   \n",
       "7     12  True   0.834601  0.603436   0.889215           0.02        5.0   \n",
       "8     28  True   0.831181  0.619244   0.833018           0.02        5.0   \n",
       "9      5  True   0.829224  0.624055   0.735924           0.02        5.0   \n",
       "\n",
       "   max_leaf_nodes  min_samples_leaf  l2_regularization  max_bins  \n",
       "0             127               200                5.0       255  \n",
       "1              63                20               10.0       255  \n",
       "2             127                50                1.0       255  \n",
       "3              63               200               10.0       255  \n",
       "4              63                20                5.0       255  \n",
       "5              63                50               10.0       255  \n",
       "6              31               100                1.0       128  \n",
       "7              63                50                0.1       128  \n",
       "8              15                20                5.0       255  \n",
       "9              15               100               10.0       128  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def fit_hgb_with_params(X_tr, y_tr, params):\n",
    "    clf = HistGradientBoostingClassifier(\n",
    "        loss=\"log_loss\",\n",
    "        random_state=42,\n",
    "        **params,\n",
    "    )\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    return clf\n",
    "\n",
    "def predict_score_hgb(model, X):\n",
    "    return model.predict_proba(X)[:, 1]\n",
    "\n",
    "def best_precision_under_min_recall(y_true, score, min_recall=0.60):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    score = np.asarray(score).astype(float)\n",
    "\n",
    "    prec, rec, thr = precision_recall_curve(y_true, score)\n",
    "    thr = np.r_[thr, 1.0]\n",
    "\n",
    "    ok = rec >= min_recall\n",
    "    if not np.any(ok):\n",
    "        return None\n",
    "\n",
    "    idx = np.argmax(np.where(ok, prec, -1.0))\n",
    "    return {\n",
    "        \"threshold\": float(thr[idx]),\n",
    "        \"precision\": float(prec[idx]),\n",
    "        \"recall\": float(rec[idx]),\n",
    "    }\n",
    "\n",
    "def sample_hgb_params(rng):\n",
    "    max_depth = rng.choice([3, 5, 7, None])\n",
    "    max_depth = None if max_depth is None else int(max_depth)\n",
    "\n",
    "    return {\n",
    "        \"learning_rate\": float(rng.choice([0.02, 0.05, 0.1])),\n",
    "        \"max_depth\": max_depth,\n",
    "        \"max_leaf_nodes\": int(rng.choice([15, 31, 63, 127])),\n",
    "        \"min_samples_leaf\": int(rng.choice([20, 50, 100, 200])),\n",
    "        \"l2_regularization\": float(rng.choice([0.0, 0.1, 1.0, 5.0, 10.0])),\n",
    "        \"max_bins\": int(rng.choice([128, 255])),\n",
    "    }\n",
    "\n",
    "\n",
    "def tune_hgb_recall_driven(X_tr, y_tr, X_te, y_te, n_trials=30, min_recall=0.60, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    rows = []\n",
    "    best = None\n",
    "\n",
    "    for t in tqdm(range(n_trials), desc=\"HGB tuning\"):\n",
    "        params = sample_hgb_params(rng)\n",
    "        model = fit_hgb_with_params(X_tr, y_tr, params)\n",
    "        score_te = predict_score_hgb(model, X_te)\n",
    "\n",
    "        best_row = best_precision_under_min_recall(y_te, score_te, min_recall=min_recall)\n",
    "\n",
    "        row = {\n",
    "            \"trial\": t,\n",
    "            \"ok\": best_row is not None,\n",
    "            \"precision\": np.nan if best_row is None else best_row[\"precision\"],\n",
    "            \"recall\": np.nan if best_row is None else best_row[\"recall\"],\n",
    "            \"threshold\": np.nan if best_row is None else best_row[\"threshold\"],\n",
    "            **params,\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "        if best_row is not None:\n",
    "            if (best is None) or (best_row[\"precision\"] > best[\"precision\"]):\n",
    "                best = {\n",
    "                    \"precision\": best_row[\"precision\"],\n",
    "                    \"recall\": best_row[\"recall\"],\n",
    "                    \"threshold\": best_row[\"threshold\"],\n",
    "                    \"params\": params,\n",
    "                    \"model\": model,\n",
    "                }\n",
    "\n",
    "    trials_df = pd.DataFrame(rows).sort_values([\"ok\", \"precision\"], ascending=[False, False]).reset_index(drop=True)\n",
    "    return best, trials_df\n",
    "\n",
    "best_hgb, hgb_trials = tune_hgb_recall_driven(\n",
    "    X_tr, y_tr, X_te, y_te,\n",
    "    n_trials=40,\n",
    "    min_recall=0.60,\n",
    ")\n",
    "\n",
    "hgb_trials.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d367a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best precision under recall constraint\n",
      "precision: 0.8496605237633366 recall: 0.6020618556701031 thr: 0.8274246241907796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9287    0.9798    0.9536      7691\n",
      "           1     0.8497    0.6021    0.7047      1455\n",
      "\n",
      "    accuracy                         0.9197      9146\n",
      "   macro avg     0.8892    0.7910    0.8292      9146\n",
      "weighted avg     0.9161    0.9197    0.9140      9146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "thr = best_hgb[\"threshold\"]\n",
    "score_te = predict_score_hgb(best_hgb[\"model\"], X_te)\n",
    "y_pred = (score_te >= thr).astype(int)\n",
    "\n",
    "print(\"best precision under recall constraint\")\n",
    "print(\"precision:\", best_hgb[\"precision\"], \"recall:\", best_hgb[\"recall\"], \"thr:\", thr)\n",
    "print(classification_report(y_te, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4308d08f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Stage1 / Stage2 모델 성능 정리\n",
    "\n",
    "## 1. Stage1 모델 (LightGBM)\n",
    "\n",
    "**Threshold 선택 기준**\n",
    "\n",
    "* Recall ≥ 0.50 조건 하에서 Precision 최대화\n",
    "\n",
    "**선택된 Threshold**\n",
    "\n",
    "* threshold = 0.990359\n",
    "* top_pct = 0.002193\n",
    "\n",
    "### Classification Report\n",
    "\n",
    "| Class | Precision | Recall | F1-score | Support |\n",
    "| ----- | --------- | ------ | -------- | ------- |\n",
    "| 0     | 0.9987    | 0.9991 | 0.9989   | 930,314 |\n",
    "| 1     | 0.6070    | 0.5074 | 0.5527   | 2,448   |\n",
    "\n",
    "**요약**\n",
    "\n",
    "* Precision (Fraud) = **0.6070**\n",
    "* Recall (Fraud) = **0.5074**\n",
    "* Accuracy = 0.9978\n",
    "\n",
    "Stage1은 전체 거래 중 극히 일부(top 0.2%)만 후보로 추출하는 1차 필터 역할을 수행한다.\n",
    "Recall을 50% 이상 확보하면서 Precision 60% 수준을 유지하는 구조이다.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Stage2 모델 (HGB, Recall-Driven Tuning)\n",
    "\n",
    "**튜닝 기준**\n",
    "\n",
    "* Recall ≥ 0.60 조건 하에서 Precision 최대화\n",
    "\n",
    "**선택 결과**\n",
    "\n",
    "* threshold = 0.8274246241907796\n",
    "* Precision = 0.8497\n",
    "* Recall = 0.6021\n",
    "\n",
    "### Classification Report\n",
    "\n",
    "| Class | Precision | Recall | F1-score | Support |\n",
    "| ----- | --------- | ------ | -------- | ------- |\n",
    "| 0     | 0.9287    | 0.9798 | 0.9536   | 7,691   |\n",
    "| 1     | 0.8497    | 0.6021 | 0.7047   | 1,455   |\n",
    "\n",
    "**요약**\n",
    "\n",
    "* Precision (Fraud) = **0.8497**\n",
    "* Recall (Fraud) = **0.6021**\n",
    "* Accuracy = 0.9197\n",
    "\n",
    "Stage2는 Stage1 통과 후보에 대해 정밀 판별을 수행하며,\n",
    "높은 Precision(85%)을 유지하면서 Recall 60% 이상을 확보하였다.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. 2-Stage 구조 전체 해석\n",
    "\n",
    "Stage1 Recall × Stage2 Recall:\n",
    "\n",
    "[\n",
    "0.5074 \\times 0.6021 \\approx 0.305\n",
    "]\n",
    "\n",
    "즉, 전체 사기 거래 중 약 **30% 수준을 최종적으로 탐지**하는 구조이다.\n",
    "\n",
    "다만 Stage2에서 Precision이 크게 상승하므로\n",
    "최종 Alert 품질은 상당히 높게 유지된다.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. 구조적 의미\n",
    "\n",
    "| 단계     | 목적       | 특징           |\n",
    "| ------ | -------- | ------------ |\n",
    "| Stage1 | 고속 1차 필터 | Recall 중심    |\n",
    "| Stage2 | 정밀 판별    | Precision 중심 |\n",
    "\n",
    "현재 구조는:\n",
    "\n",
    "* Stage1에서 후보 압축\n",
    "* Stage2에서 고정밀 판별\n",
    "\n",
    "이라는 설계 목적에 부합하는 성능을 보인다.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU-CAT)",
   "language": "python",
   "name": "gpu-cat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
